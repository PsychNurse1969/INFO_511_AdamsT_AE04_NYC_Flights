{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0cfba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "INFO_511_ Application Exercise 04: NYC Flights\n",
    "Author: Todd Adams\n",
    "Date: 04/06/2024\n",
    "Description: We are answering questions related to the NYC Flights dataset.\n",
    "Note: I used VS Code and ChatGPT to help me write this code.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8a471",
   "metadata": {},
   "source": [
    "**Load Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "from nycflights13 import flights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78233f7",
   "metadata": {},
   "source": [
    "**Exercise 1 - Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the flights dataset\n",
    "flights = pd.read_csv(\"data/flights.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset and its info\n",
    "flights.head()\n",
    "flights.info()\n",
    "flights.describe()\n",
    "\n",
    "# Count the number of rows in the dataset\n",
    "flights_df = flights.copy()\n",
    "len(flights_df)\n",
    "\n",
    "# Number of rows\n",
    "num_rows = len(flights)\n",
    "print(f\"The flights dataset has {num_rows} rows.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea457de",
   "metadata": {},
   "source": [
    "The `flights` data frame has `336776` rows. Each row represents a `single flight departing NYC in 2013`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d995189a",
   "metadata": {},
   "source": [
    "**Exercise 2 - Data Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f52365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values in 'arr_delay' and 'distance'\n",
    "flights_clean = flights.dropna(subset=['arr_delay', 'distance'])\n",
    "flights_clean.columns.tolist()\n",
    "\n",
    "# Display how many rows were originally in the dataset and how many are left after cleaning\n",
    "print(f\"Original rows: {len(flights)}\")\n",
    "print(f\"Cleaned rows: {len(flights_clean)}\")\n",
    "\n",
    "# View column names\n",
    "flights_clean.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab4413",
   "metadata": {},
   "source": [
    "**Exercise 3 - Original Data Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plot style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create side-by-side histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram for arr_delay\n",
    "sns.histplot(data=flights, x=\"arr_delay\", bins=100, kde=False, ax=axes[0], color='skyblue')\n",
    "axes[0].set_title(\"Arrival Delay Distribution\")\n",
    "axes[0].set_xlabel(\"Arrival Delay (minutes)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Histogram for distance\n",
    "sns.histplot(data=flights, x=\"distance\", bins=50, kde=False, ax=axes[1], color='salmon')\n",
    "axes[1].set_title(\"Flight Distance Distribution\")\n",
    "axes[1].set_xlabel(\"Distance (miles)\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d189b",
   "metadata": {},
   "source": [
    "**Exercise 4 - Check for Skewness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2678f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate skewness\n",
    "arr_delay_skew = flights['arr_delay'].skew()\n",
    "distance_skew = flights['distance'].skew()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Skewness of arr_delay: {arr_delay_skew:.2f}\")\n",
    "print(f\"Skewness of distance: {distance_skew:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d660e",
   "metadata": {},
   "source": [
    "**Exercise  - Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e4ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics to check for scale differences\n",
    "flights_clean[['arr_delay', 'distance']].describe()\n",
    "\n",
    "'''\n",
    "Yes, both `arr_delay` and `distance` need to be scaled. While they are both numeric, they are measured on **very different scales**:  \n",
    "`arr_delay` ranges in minutes and includes negative/positive values, while `distance` can span from short to long-haul flights (e.g., 100–3000+ miles).  \n",
    "Scaling ensures that models or plots using both features give them **equal weight** and aren’t biased by magnitude.  \n",
    "'''\n",
    "\n",
    "# Create copies to avoid modifying the original\n",
    "df_clean = flights_clean.copy()\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_standard = StandardScaler()\n",
    "scaler_maxabs = MaxAbsScaler()\n",
    "scaler_minmax = MinMaxScaler()\n",
    "\n",
    "# Fit and transform\n",
    "df_clean['arr_delay_standard'] = scaler_standard.fit_transform(df_clean[['arr_delay']])\n",
    "df_clean['distance_standard'] = scaler_standard.fit_transform(df_clean[['distance']])\n",
    "\n",
    "df_clean['arr_delay_maxabs'] = scaler_maxabs.fit_transform(df_clean[['arr_delay']])\n",
    "df_clean['distance_maxabs'] = scaler_maxabs.fit_transform(df_clean[['distance']])\n",
    "\n",
    "df_clean['arr_delay_minmax'] = scaler_minmax.fit_transform(df_clean[['arr_delay']])\n",
    "df_clean['distance_minmax'] = scaler_minmax.fit_transform(df_clean[['distance']])\n",
    "\n",
    "# View the results\n",
    "df_clean[['arr_delay_standard', 'distance_standard', 'arr_delay_maxabs', 'distance_maxabs', 'arr_delay_minmax', 'distance_minmax']].describe()\n",
    "\n",
    "'''\n",
    "What are two pros and two cons of standardizing data?\n",
    "**Pros:**\n",
    "1. Standardization ensures variables contribute equally to distance-based algorithms (like KNN, clustering).\n",
    "2. It helps improve the performance and convergence of gradient-based models (like linear regression, logistic regression, etc.).\n",
    "\n",
    "**Cons:**\n",
    "1. It may obscure the original scale, making interpretation of coefficients and outputs less intuitive.\n",
    "2. If applied improperly (e.g., on categorical or already-normalized data), it can distort relationships and reduce model performance.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff57e2",
   "metadata": {},
   "source": [
    "**Exercise 6 - Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a10b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check summary statistics again for scaled variables\n",
    "df_clean[['arr_delay_minmax', 'distance_minmax']].describe()\n",
    "\n",
    "'''\n",
    " Why should you use the min-max scaled data instead of a different scaling for the transformations?  \n",
    " \n",
    " Min-Max scaling ensures all values fall within a **[0, 1]** range. This is important because:\n",
    "- Log and square root transformations are **sensitive to negative values**, which could cause math errors.\n",
    "- Min-Max scaling guarantees non-negative values, which is **required** for log and sqrt to work correctly.\n",
    "- It also preserves the relationships between values, making it suitable for algorithms sensitive to the scale of data.\n",
    "- This is especially important for algorithms like KNN, SVM, and neural networks that rely on distance metrics.\n",
    "'''\n",
    "\n",
    "# Check skewness first\n",
    "arr_skew = df_clean['arr_delay'].skew()\n",
    "dist_skew = df_clean['distance'].skew()\n",
    "\n",
    "# Apply transformations based on skew direction\n",
    "if arr_skew > 0:\n",
    "    # Positive skew: log transformation (add constant to avoid log(0))\n",
    "    df_clean['arr_delay_log'] = np.log1p(df_clean['arr_delay_minmax'])  # log(1 + x)\n",
    "else:\n",
    "    df_clean['arr_delay_log'] = df_clean['arr_delay_minmax']  # no transformation needed\n",
    "\n",
    "if dist_skew < 0:\n",
    "    # Negative skew: sqrt transformation\n",
    "    df_clean['distance_sqrt'] = np.sqrt(df_clean['distance_minmax'])\n",
    "else:\n",
    "    df_clean['distance_sqrt'] = df_clean['distance_minmax']  # no transformation needed\n",
    "\n",
    "'''\n",
    " Why do we have to add a constant when we perform a log or square-root transformation (i.e., np.log1p(df['column' + 1]))?\n",
    "\n",
    "We add a constant (usually 1) because:\n",
    "- You **can’t take the log or square root of 0 or negative numbers** — it’s mathematically undefined.\n",
    "- Adding 1 (as in `np.log1p(x)` or `np.sqrt(x + 1)`) shifts all values just enough to avoid errors while keeping the relative scale of the data.\n",
    "- It also ensures that the transformation is **smooth** and **continuous**,  \n",
    "which is important for many statistical methods and machine learning algorithms.\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
